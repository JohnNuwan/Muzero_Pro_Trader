# 06 - EntraÃ®nement Hybride

## ðŸ“š Introduction

V19 utilise un **entraÃ®nement hybride** mixant donnÃ©es synthÃ©tiques (self-play) et rÃ©elles.

---

## ðŸ”€ Mixing Ratio

```
Batch = 60% Self-Play + 40% Real Trades
```

### Justification

- **60% Self-Play** : Exploration, diversitÃ© de stratÃ©gies
- **40% Real Trades** : Grounding, rÃ©alitÃ© du marchÃ©

---

## ðŸŽ¯ Construction des Batchs

```python
batch_size = 64
sp_batch_size = int(64 * 0.6) = 38
real_batch_size = int(64 * 0.4) = 26

# Sample
sp_indices = np.random.choice(len(self_play_data), 38)
real_indices = np.random.choice(len(real_data), 26)

# Concatenate
batch_states = np.concatenate([
    self_play_states[sp_indices],
    real_states[real_indices]
])
```

---

## ðŸ“Š Training Loop

```python
for epoch in range(300):
    for batch in iterate_batches():
        # Forward
        policy_pred, value_pred = model(batch_states)
        
        # Loss
        loss = policy_loss(policy_pred, target_policy) + \
               value_loss(value_pred, target_value)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

---

## ðŸŽ¯ Loss Functions

### Policy Loss

```
L_Ï€ = -Î£_a Ï€_target(a) * log(Ï€_pred(a) + Îµ)
```

### Value Loss

```
L_V = (V_target - V_pred)Â²
```

### Total

```
L = L_Ï€ + L_V + Î» * ||Î¸||Â²
```

---

**Prochaine section** : [07_Tournament.md](07_Tournament.md)
