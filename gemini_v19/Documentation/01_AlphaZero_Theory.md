# 01 - ThÃ©orie AlphaZero

## ðŸ“š Introduction

AlphaZero est un algorithme d'apprentissage par renforcement rÃ©volutionnaire dÃ©veloppÃ© par DeepMind en 2017. Il combine trois concepts puissants :

1. **Monte Carlo Tree Search (MCTS)** - Exploration efficace de l'espace des dÃ©cisions
2. **Deep Neural Networks** - Approximation de la politique et de la valeur
3. **Self-Play** - Apprentissage autonome sans donnÃ©es Ã©tiquetÃ©es

---

## ðŸ§® Fondements MathÃ©matiques

### 1. Processus de DÃ©cision Markovien (MDP)

Le trading est modÃ©lisÃ© comme un **MDP** dÃ©fini par le tuple **(S, A, P, R, Î³)** :

- **S** : Espace des Ã©tats (observations du marchÃ©)
- **A** : Espace des actions {HOLD, BUY, SELL, SPLIT, CLOSE}
- **P** : Fonction de transition P(s'|s,a) (stochastique)
- **R** :  Fonction de rÃ©compense R(s,a,s')
- **Î³** : Facteur de discount (Î³ âˆˆ [0,1])

**Ã‰quation de Bellman** :

```
V^Ï€(s) = E_Ï€ [ Î£(t=0 to âˆž) Î³^t * r_t | s_0 = s ]
```

OÃ¹ :
- **V^Ï€(s)** est la valeur de l'Ã©tat s sous la politique Ï€
- **r_t** est la rÃ©compense au temps t

### 2. Fonction de Politique

La politique **Ï€(a|s)** est une distribution de probabilitÃ© sur les actions :

```
Ï€(a|s) = P(A_t = a | S_t = s)
```

**Objectif** : Trouver la politique optimale **Ï€*** qui maximise le retour espÃ©rÃ© :

```
Ï€* = argmax_Ï€ E[Î£ Î³^t * r_t]
```

### 3. Fonction Action-Valeur (Q-Function)

La Q-function **Q^Ï€(s,a)** estime le retour espÃ©rÃ© en prenant l'action a dans l'Ã©tat s, puis en suivant Ï€ :

```
Q^Ï€(s,a) = E_Ï€ [ r + Î³ * V^Ï€(s') ]
```

**Ã‰quation de Bellman pour Q** :

```
Q*(s,a) = E [ r + Î³ * max_a' Q*(s', a') ]
```

---

## ðŸŽ® Paradigme Self-Play

### Concept

Au lieu d'apprendre Ã  partir de donnÃ©es Ã©tiquetÃ©es, AlphaZero **joue contre lui-mÃªme** :

1. Le modÃ¨le actuel gÃ©nÃ¨re des parties
2. Ces parties servent de donnÃ©es d'entraÃ®nement
3. Un nouveau modÃ¨le est entraÃ®nÃ© sur ces donnÃ©es
4. Le nouveau modÃ¨le remplace l'ancien s'il est meilleur

### Avantages

- âœ… **Pas de biais humain** : Le systÃ¨me dÃ©couvre des stratÃ©gies novatrices
- âœ… **DonnÃ©es infinies** : GÃ©nÃ©ration continue de nouvelles parties
- âœ… **AmÃ©lioration garantie** : Validation par tournoi

---

## ðŸ§  Architecture de RÃ©seau

AlphaZero utilise un **rÃ©seau dual-head** :

### Input Layer
```
Ã‰tat s âˆˆ R^84
```

### Shared Trunk
```
h = ReLU(BatchNorm(Linear(s, 256)))
h = ReLU(BatchNorm(Linear(h, 256)))
h = ReLU(BatchNorm(Linear(h, 256)))
```

### Policy Head (Ï€)
```
p_logits = Linear(h, 128)
p_logits = ReLU(p_logits)
p_logits = Linear(p_logits, 5)
Ï€(a|s) = Softmax(p_logits)
```

**Output** : Distribution de probabilitÃ© sur les 5 actions

### Value Head (V)
```
v = Linear(h, 128)
v = ReLU(v)
v = Linear(v, 1)
V(s) = Tanh(v)
```

**Output** : Valeur estimÃ©e de l'Ã©tat âˆˆ [-1, 1]

---

## ðŸ“Š Fonction de Perte (Loss Function)

AlphaZero minimise une loss composite :

```
L(Î¸) = (z - v_Î¸(s))Â² - Ï€^T log p_Î¸(s) + c * ||Î¸||Â²
```

OÃ¹ :
- **z** : Valeur cible (Monte Carlo return ou reward rÃ©el)
- **v_Î¸(s)** : PrÃ©diction de valeur du rÃ©seau
- **Ï€** : Politique cible (vecteur de probabilitÃ©s)
- **p_Î¸(s)** : PrÃ©diction de politique du rÃ©seau
- **c** : Coefficient de rÃ©gularisation L2
- **Î¸** : ParamÃ¨tres du rÃ©seau

### Composantes

#### 1. Value Loss (MSE)
```
L_value = (z - v_Î¸(s))Â²
```

Minimise l'erreur quadratique entre la valeur prÃ©dite et la cible.

#### 2. Policy Loss (Cross-Entropy)
```
L_policy = -Î£_a Ï€(a) * log p_Î¸(a|s)
```

Maximise la log-likelihood de la politique cible.

#### 3. Regularization Loss
```
L_reg = c * Î£ Î¸_iÂ²
```

PrÃ©vient l'overfitting en pÃ©nalisant les grands poids.

---

## ðŸ”„ Algorithme de Self-Play

### Pseudocode

```python
def self_play():
    state = env.reset()
    trajectory = []
    
    while not done:
        # MCTS Search
        policy, value = mcts_search(state, network, n_sims=50)
        
        # Sample action
        action = sample(policy)
        
        # Execute
        next_state, reward, done = env.step(action)
        
        # Store
        trajectory.append((state, policy, reward))
        state = next_state
    
    # Backpropagate returns
    returns = compute_monte_carlo_returns(trajectory)
    
    return [(s, Ï€, G) for (s, Ï€, r), G in zip(trajectory, returns)]
```

### Compute Monte Carlo Returns

```python
def compute_monte_carlo_returns(trajectory, gamma=0.99):
    G = 0
    returns = []
    
    for (s, Ï€, r) in reversed(trajectory):
        G = r + gamma * G
        returns.insert(0, G)
    
    return returns
```

---

## ðŸŽ¯ Ã‰quation PUCT (Predictor + UCT)

Pendant la recherche MCTS, chaque nÅ“ud est Ã©valuÃ© avec :

```
Score(s,a) = Q(s,a) + U(s,a)
```

OÃ¹ :

```
U(s,a) = c_puct * P(s,a) * âˆš(Î£_b N(s,b)) / (1 + N(s,a))
```

**Termes** :
- **Q(s,a)** : Valeur moyenne de l'action a (exploitation)
- **P(s,a)** : ProbabilitÃ© a priori de la politique rÃ©seau
- **N(s,a)** : Nombre de visites de (s,a)
- **c_puct** : Constante d'exploration (1.5 dans V19)

**PropriÃ©tÃ©s** :
- Actions peu visitÃ©es ont un U(s,a) Ã©levÃ© â†’ Exploration
- Actions avec haute probabilitÃ© P(s,a) sont favorisÃ©es
- Balance exploration-exploitation de faÃ§on optimale

---

## ðŸ“ˆ Convergence et Garanties

### ThÃ©orÃ¨me (Silver et al., 2017)

Sous certaines conditions (ergodicity, sufficient exploration), la suite de politiques gÃ©nÃ©rÃ©e par AlphaZero converge vers un **Nash Equilibrium** du jeu.

### En pratique pour Trading

Le marchÃ© n'est pas un jeu Ã  somme nulle et non-stationnaire, donc :
- âŒ Pas de garantie de convergence stricte
- âœ… AmÃ©lioration continue observÃ©e empiriquement
- âœ… Validation par tournoi assure non-rÃ©gression

---

## ðŸ”¬ Innovations de V19

### 1. Hybrid Training (MuZero-inspired)

Au lieu de seulement du self-play, V19 mixe :
- **60% Self-Play** : Exploration de nouvelles stratÃ©gies
- **40% Real Trades** : Grounding dans la rÃ©alitÃ© du marchÃ©

**Formule** :

```
Batch = Sample(60%, Self_Play_Buffer) âˆª Sample(40%, Real_Trades_Buffer)
```

### 2. Multi-Timeframe State

State vector intÃ¨gre 6 timeframes (M1, M5, M15, H1, H4, D1) :

```
s = [f_M1, f_M5, f_M15, f_H1, f_H4, f_D1, t_features, pos_features]
```

OÃ¹ **f_tf** est un vecteur de 13 indicateurs techniques par timeframe.

### 3. Asymmetric Reward

Les pertes pÃ¨sent 2Ã— plus que les gains :

```
R(s,a,s') = {
    PnL           si PnL > 0
    2 * PnL       si PnL < 0
}
```

**Justification** : Mimique l'aversion au risque humaine et incentivise les trades de qualitÃ©.

---

## ðŸ“š RÃ©fÃ©rences

1. **Silver, D., et al. (2017)** - "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"
2. **Schrittwieser, J., et al. (2020)** - "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model" (MuZero)
3. **Sutton & Barto (2018)** - "Reinforcement Learning: An Introduction"

---

**Prochaine section** : [02_MCTS_Algorithm.md](02_MCTS_Algorithm.md)
