# 02 - Algorithme MCTS (Monte Carlo Tree Search)

## üìö Introduction

Le **Monte Carlo Tree Search** est l'algorithme central de d√©cision dans AlphaZero. Il combine :
- Recherche arborescente guid√©e par heuristique
- √âvaluation Monte Carlo (sampling)
- R√©seau neuronal pour l'initialisation et l'√©valuation

---

## üå≥ Structure de l'Arbre

### N≈ìud MCTS

Chaque n≈ìud repr√©sente un **√©tat du jeu** (ou √©tat du march√© en trading).

**Attributs** :
```python
class MCTSNode:
    state: np.ndarray        # √âtat observ√© (84 features)
    parent: MCTSNode | None  # N≈ìud parent
    children: dict[int, MCTSNode]  # Actions ‚Üí N≈ìuds enfants
    
    visit_count: int         # N(s)
    value_sum: float         # W(s)
    prior_p: float           # P(s,a) - Proba a priori de la policy
```

**Valeur moyenne Q** :

```
Q(s) = W(s) / N(s)
```

O√π :
- **W(s)** : Somme cumul√©e des valeurs backpropag√©es
- **N(s)** : Nombre total de visites

---

## üîÑ Les 4 Phases de MCTS

### 1Ô∏è‚É£ Selection

**Objectif** : Descendre dans l'arbre jusqu'√† une feuille non-√©tendue.

**Algorithme** :
```python
def select(node):
    while node.is_expanded():
        action, node = select_child(node, c_puct=1.5)
    return node
```

**Crit√®re PUCT** (Predictor + UCT) :

```
a* = argmax_a [ Q(s,a) + U(s,a) ]
```

O√π :

```
U(s,a) = c_puct * P(s,a) * ‚àö(Œ£_b N(s,b)) / (1 + N(s,a))
```

**D√©composition** :

- **Q(s,a)** : Exploitation - Valeur moyenne observ√©e
  ```
  Q(s,a) = W(s,a) / N(s,a)
  ```

- **U(s,a)** : Exploration - Bonus pour actions peu visit√©es
  - ‚Üë si **P(s,a)** √©lev√©e (confiance du r√©seau)
  - ‚Üë si **N(s,a)** faible (peu explor√©)
  - ‚Üì quand le parent est tr√®s visit√© (exploration normalis√©e)

**Code V19** :

```python
def select_child(node, c_puct):
    best_score = -np.inf
    best_action = None
    best_child = None
    
    # Total visits parent
    total_n = sum(child.visit_count for child in node.children.values())
    
    for action, child in node.children.items():
        # Q value
        q_value = child.value_sum / child.visit_count if child.visit_count > 0 else 0
        
        # U value (exploration bonus)
        u_value = c_puct * child.prior_p * np.sqrt(total_n) / (1 + child.visit_count)
        
        # PUCT score
        score = q_value + u_value
        
        if score > best_score:
            best_score = score
            best_action = action
            best_child = child
    
    return best_action, best_child
```

---

### 2Ô∏è‚É£ Expansion

**Objectif** : Cr√©er les n≈ìuds enfants pour une feuille.

**Processus** :

1. Pr√©dire la politique avec le r√©seau neuronal :
   ```
   œÄ(¬∑|s), V(s) = Network_Œ∏(s)
   ```

2. Pour chaque action l√©gale **a ‚àà A** :
   - Simuler `s' = Env.step(s, a)`
   - Cr√©er n≈ìud enfant avec `prior_p = œÄ(a|s)`

**Code V19** :

```python
def expand(node, network, env):
    # Network inference
    with torch.no_grad():
        policy_logits, value = network(torch.FloatTensor([node.state]))
        policy = policy_logits[0].numpy()  # Softmax already applied
        value = value.item()
    
    # Create children for all valid actions
    for action in range(5):  # HOLD, BUY, SELL, SPLIT, CLOSE
        # Clone env to simulate
        child_env = copy.deepcopy(env)
        next_state, _, done, _, _ = child_env.step(action)
        
        if not done:
            node.children[action] = MCTSNode(
                state=next_state,
                parent=node,
                prior_p=policy[action],
                action=action
            )
    
    return value
```

---

### 3Ô∏è‚É£ Simulation / Evaluation

Dans AlphaZero, la simulation est **remplac√©e par le Value Head du r√©seau** :

```
V(s_leaf) ‚âà E[ Œ£ Œ≥^t r_t | s_leaf ]
```

**Avantage** :
- ‚ùå Pas besoin de rollouts al√©atoires (lent et bruit√©)
- ‚úÖ √âvaluation apprise par le r√©seau (rapide et pr√©cise)

**Dans V19** :

```python
value = network.value_head(node.state)  # ‚àà [-1, 1]
```

Cette valeur est ensuite backpropag√©e.

---

### 4Ô∏è‚É£ Backpropagation

**Objectif** : Remonter la valeur jusqu'√† la racine et mettre √† jour les statistiques.

**Algorithme** :

```python
def backpropagate(search_path, value):
    for node in reversed(search_path):
        node.visit_count += 1
        node.value_sum += value
        # Note: Pas de flip de signe car trading = single player
```

**Mise √† jour des Q-values** :

Apr√®s backprop, les Q-values sont automatiquement recalcul√©es :

```
Q(s,a) = W(s,a) / N(s,a)
```

---

## üé≤ Dirichlet Noise (Exploration Root)

Pour encourager l'exploration au n≈ìud racine, AlphaZero ajoute du **bruit de Dirichlet** :

```
P_root(a) = (1 - Œµ) * œÄ(a) + Œµ * Œ∑_a
```

O√π :
- **Œµ** : Fraction du bruit (0.25 dans V19)
- **Œ∑** ~ **Dir(Œ±)** : Vecteur Dirichlet avec Œ± = 0.3

**Propri√©t√©s Dirichlet** :

```
Œ∑ ~ Dir(Œ±‚ÇÅ, ..., Œ±_K)
    Œ£ Œ∑_i = 1
    Œ∑_i ‚àà [0, 1]
```

Pour **Œ± < 1** (sparse) : Certains Œ∑_i sont proches de 1, d'autres proches de 0  
Pour **Œ± > 1** (dense) : Œ∑_i uniform√©ment r√©partis

**Code V19** :

```python
def add_dirichlet_noise(policy, alpha=0.3, epsilon=0.25):
    noise = np.random.dirichlet([alpha] * len(policy))
    return (1 - epsilon) * policy + epsilon * noise
```

---

## üìä Policy de Sortie

Apr√®s **N simulations**, la politique finale est d√©riv√©e des **visit counts** :

```
œÄ(a|s_root) = N(s_root, a)^(1/œÑ) / Œ£_b N(s_root, b)^(1/œÑ)
```

O√π **œÑ** est la **temp√©rature** :

- **œÑ ‚Üí 0** : D√©terministe, œÄ(a*) = 1 o√π a* = argmax N(s,a)
- **œÑ = 1** : Proportionnel aux visites, œÄ(a) ‚àù N(s,a)
- **œÑ > 1** : Plus uniforme (plus d'exploration)

**Dans V19** :

- **Training** : œÑ = 1.0 (stochastique pour exploration)
- **Evaluation** : œÑ = 0.1 (quasi-d√©terministe)

**Code** :

```python
def compute_policy(root, temperature=1.0):
    visit_counts = np.array([child.visit_count for child in root.children.values()])
    actions = list(root.children.keys())
    
    if temperature == 0:
        # Deterministic
        best_idx = np.argmax(visit_counts)
        policy = np.zeros(len(actions))
        policy[best_idx] = 1.0
    else:
        # Stochastic
        visit_counts = visit_counts ** (1.0 / temperature)
        policy = visit_counts / np.sum(visit_counts)
    
    # Map to full action space
    full_policy = np.zeros(5)
    for i, action in enumerate(actions):
        full_policy[action] = policy[i]
    
    return full_policy
```

---

## ‚öôÔ∏è Param√®tres MCTS V19

| Param√®tre | Valeur | Description |
|-----------|--------|-------------|
| `n_simulations` | 50 | Nombre de simulations par search |
| `c_puct` | 1.5 | Constante d'exploration PUCT |
| `dirichlet_alpha` | 0.3 | Param√®tre du bruit Dirichlet |
| `exploration_fraction` | 0.25 | Fraction du bruit au root (Œµ) |
| `temperature` | 1.0 / 0.1 | Train / Eval |

---

## üî¨ Complexit√© et Performance

### Complexit√© Temporelle

Pour **N** simulations et **b** branches par n≈ìud (‚âà5 dans V19) :

- **Selection** : O(depth) ‚âà O(log N)
- **Expansion** : O(b) = O(5) = O(1)
- **Evaluation** : O(1) (forward pass r√©seau)
- **Backpropagation** : O(depth) ‚âà O(log N)

**Total par simulation** : O(log N)

**Total pour N simulations** : O(N log N)

### Complexit√© Spatiale

Arbre MCTS : O(N √ó b) ‚âà O(250) n≈ìuds pour N=50, b=5

### Temps R√©el V19

- **50 MCTS sims** : ~55 secondes
- **1 forward pass** : ~10 ms (CPU)
- **Bottleneck** : Environment stepping (copy.deepcopy)

---

## üéØ Comparaison MCTS vs Minimax

| Aspect | MCTS | Minimax |
|--------|------|---------|
| **Exploration** | Selective (PUCT) | Exhaustive |
| **Heuristique** | R√©seau neuronal | Fonction d'√©valuation manuelle |
| **Profondeur** | Adaptive | Fixe ou iterative deepening |
| **Complexit√©** | O(N log N) | O(b^d) exponential |
| **Trading** | ‚úÖ Excellent | ‚ùå Trop lent |

---

## üìà Convergence de MCTS

### Th√©or√®me (Kocsis & Szepesv√°ri, 2006)

Avec UCB1 (anc√™tre de PUCT), MCTS converge vers la meilleure action avec probabilit√© 1 quand **N ‚Üí ‚àû**.

### En Pratique

- **N = 50** dans V19 est suffisant pour des d√©cisions fiables
- Trade-off exploration-exploitation bien calibr√© avec c_puct = 1.5
- Dirichlet noise √©vite les modes locaux

---

## üöÄ Optimisations V19

### 1. Lazy Expansion

Cr√©er les enfants seulement quand visit√©s :

```python
if action not in node.children:
    node.children[action] = create_child(node, action)
```

**Gain** : R√©duit m√©moire et temps si certaines branches jamais explor√©es.

### 2. Virtual Loss (Multi-threading)

Pour parall√©liser MCTS, ajouter une perte virtuelle :

```python
node.virtual_loss += 1  # Avant simulation
# ... simulation ...
node.virtual_loss -= 1  # Apr√®s backprop
```

**Effet** : D√©courage les threads de suivre le m√™me chemin simultan√©ment.

*Note : V19 n'utilise pas cette optimisation (single-threaded).*

---

## üîó Lien avec le R√©seau

Le r√©seau neuronal **guide** MCTS via :

1. **Policy Head** ‚Üí Priors P(s,a)
   - Initialise les probabilit√©s avant exploration
   - R√©duit l'espace de recherche

2. **Value Head** ‚Üí Evaluation V(s)
   - Remplace les rollouts al√©atoires
   - Apprentissage par self-play am√©liore progressivement

**Boucle Vertueuse** :

```
Meilleur R√©seau ‚Üí Meilleure Guidance MCTS ‚Üí Meilleurs Donn√©es ‚Üí Meilleur R√©seau
```

---

**Prochaine section** : [03_Network_Architecture.md](03_Network_Architecture.md)
