# 03 - Architecture du R√©seau Neuronal

## üìö Introduction

Le r√©seau neuronal **AlphaZeroTradingNet** est au c≈ìur du syst√®me V19. C'est un r√©seau **dual-head** qui pr√©dit simultan√©ment :

1. **Policy œÄ(a|s)** - Distribution de probabilit√© sur les actions
2. **Value V(s)** - Estimation de la valeur de l'√©tat

---

## üèóÔ∏è Architecture Globale

```
Input (84,)
    ‚Üì
[Shared Trunk: 3√ó256 MLP + BatchNorm + ReLU + Dropout]
    ‚Üì
    ‚îú‚îÄ‚îÄ‚Üí [Policy Head] ‚Üí œÄ(a|s) ‚àà R^5 (proba sur 5 actions)
    ‚îî‚îÄ‚îÄ‚Üí [Value Head]  ‚Üí V(s) ‚àà [-1, 1] (scalar)
```

---

## üì• Input Layer

### Input Dimensions: **84 features**

D√©composition :

```
84 = 6 timeframes √ó 13 indicators + 4 time + 2 position

= 78 (multi-timeframe indicators)
+ 4  (temporal encoding)
+ 2  (position state)
```

#### 1. Multi-Timeframe Indicators (78)

Pour chaque timeframe (M1, M5, M15, H1, H4, D1), **13 indicateurs** :

```python
features_per_tf = [
    'rsi',           # Relative Strength Index
    'mfi',           # Money Flow Index
    'adx',           # Average Directional Index
    'z_score',       # Statistical Z-score
    'trend_score',   # Proprietary trend indicator
    'linreg_angle',  # Linear regression angle
    'fibo_pos',      # Position relative to Fibonacci levels
    'dist_to_res',   # Distance to resistance
    'dist_to_sup',   # Distance to support
    'skew',          # Statistical skewness
    'kurtosis',      # Statistical kurtosis
    'entropy',       # Shannon entropy
    'hurst'          # Hurst exponent
]
```

**Total** : 6 √ó 13 = 78 features

#### 2. Temporal Features (4)

Encodage cyclique du temps :

```python
hour = current_time.hour
day = current_time.dayofweek

time_features = [
    sin(2œÄ * hour / 24),      # Heure (cyclique)
    cos(2œÄ * hour / 24),
    sin(2œÄ * day / 7),        # Jour semaine (cyclique)
    cos(2œÄ * day / 7)
]
```

**Pourquoi cyclique ?** Pour capturer la p√©riodicit√© (17h ‚âà 18h, mais 23h ‚âà 0h).

#### 3. Position State (2)

```python
pos_state = {
    -1.0  si position SHORT
     0.0  si position FLAT
    +1.0  si position LONG
}

pnl_pct = (current_price - entry_price) / entry_price  si position != 0
          0.0                                           sinon
```

**Total** : 2 features

---

## üß± Shared Trunk

Le **Shared Trunk** est un MLP √† 3 couches hidden, partag√© entre les deux heads.

### Architecture

```python
self.shared_trunk = nn.Sequential(
    nn.Linear(84, 256),
    nn.BatchNorm1d(256),
    nn.ReLU(),
    nn.Dropout(0.1),
    
    nn.Linear(256, 256),
    nn.BatchNorm1d(256),
    nn.ReLU(),
    nn.Dropout(0.1),
    
    nn.Linear(256, 256),
    nn.BatchNorm1d(256),
    nn.ReLU(),
    nn.Dropout(0.1)
)
```

### Forward Pass

```
h‚ÇÄ = Input         # (batch, 84)
h‚ÇÅ = ReLU(BN(W‚ÇÅ h‚ÇÄ + b‚ÇÅ)) + Dropout
h‚ÇÇ = ReLU(BN(W‚ÇÇ h‚ÇÅ + b‚ÇÇ)) + Dropout
h‚ÇÉ = ReLU(BN(W‚ÇÉ h‚ÇÇ + b‚ÇÉ)) + Dropout
```

Output : **h‚ÇÉ ‚àà R^256** (shared representation)

### Batch Normalization

**Formule** :

```
BN(x) = Œ≥ * (x - Œº) / ‚àö(œÉ¬≤ + Œµ) + Œ≤
```

O√π :
- **Œº, œÉ¬≤** : Mean et variance du batch
- **Œ≥, Œ≤** : Param√®tres apprenables (scale & shift)
- **Œµ** : Petite constante pour stabilit√© num√©rique (1e-5)

**Avantages** :
- ‚úÖ Acc√©l√®re la convergence
- ‚úÖ R√©gularisation (effet similaire au dropout)
- ‚úÖ Permet learning rates plus √©lev√©s

### Dropout

**Formule** :

```
Dropout(x, p=0.1) = {
    x / (1-p)  avec probabilit√© (1-p)
    0          avec probabilit√© p
}
```

**Effet** :
- **Training** : D√©sactive al√©atoirement 10% des neurones
- **Inference** : Pas de dropout (mode eval)

**Avantage** : Pr√©vient l'overfitting en for√ßant redundancy.

---

## üé≠ Policy Head

Le **Policy Head** pr√©dit une distribution de probabilit√© sur les actions.

### Architecture

```python
self.policy_head = nn.Sequential(
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 5)
    # Softmax appliqu√© dans forward()
)
```

### Forward Pass

```
p_logits = W_p2 * ReLU(W_p1 * h‚ÇÉ + b_p1) + b_p2
œÄ(a|s) = Softmax(p_logits)
```

**Softmax** :

```
œÄ(a|s) = exp(p_logits[a]) / Œ£_i exp(p_logits[i])
```

**Propri√©t√©s** :
- Œ£_a œÄ(a|s) = 1 (distribution de probabilit√© valide)
- œÄ(a|s) ‚àà [0, 1] pour tout a
- Max likelihood training via cross-entropy

### Output Dimensions

**œÄ(a|s) ‚àà R^5** :

```python
actions = {
    0: HOLD    # Pas de changement
    1: BUY     # Ouvrir ou pyramider long
    2: SELL    # Ouvrir ou pyramider short
    3: SPLIT   # Fermer 50% de la position
    4: CLOSE   # Fermer 100% de la position
}
```

---

## üí∞ Value Head

Le **Value Head** pr√©dit la valeur esp√©r√©e de l'√©tat.

### Architecture

```python
self.value_head = nn.Sequential(
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Linear(128, 1),
    nn.Tanh()
)
```

### Forward Pass

```
v = W_v2 * ReLU(W_v1 * h‚ÇÉ + b_v1) + b_v2
V(s) = Tanh(v)
```

**Tanh** :

```
Tanh(x) = (e^x - e^-x) / (e^x + e^-x)
```

**Propri√©t√©s** :
- V(s) ‚àà [-1, 1]
- Sym√©trique autour de 0
- Saturation pour grandes valeurs (|x| > 3)

### Interpr√©tation

- **V(s) ‚âà +1** : √âtat tr√®s favorable (gains attendus)
- **V(s) ‚âà 0** : √âtat neutre
- **V(s) ‚âà -1** : √âtat d√©favorable (pertes attendues)

---

## üìä Nombre de Param√®tres

### Calcul

| Layer | Input | Output | Weights | Biases | BatchNorm | **Total** |
|-------|-------|--------|---------|--------|-----------|-----------|
| Linear1 | 84 | 256 | 21,504 | 256 | 512 | **22,272** |
| Linear2 | 256 | 256 | 65,536 | 256 | 512 | **66,304** |
| Linear3 | 256 | 256 | 65,536 | 256 | 512 | **66,304** |
| Policy1 | 256 | 128 | 32,768 | 128 | 0 | **32,896** |
| Policy2 | 128 | 5 | 640 | 5 | 0 | **645** |
| Value1 | 256 | 128 | 32,768 | 128 | 0 | **32,896** |
| Value2 | 128 | 1 | 128 | 1 | 0 | **129** |

**Total** : ~221,446 param√®tres

---

## üîÑ Forward Pass Complet

```python
def forward(self, state):
    # Input validation
    if not isinstance(state, torch.Tensor):
        state = torch.FloatTensor(state)
    
    # Shared trunk
    features = self.shared_trunk(state)  # (batch, 256)
    
    # Policy head
    policy_logits = self.policy_head(features)  # (batch, 5)
    policy = F.softmax(policy_logits, dim=-1)
    
    # Value head
    value = self.value_head(features)  # (batch, 1)
    
    return policy, value
```

**Dimensions** :

```
Input:    (batch, 84)
          ‚Üì
Features: (batch, 256)
          ‚Üì
Policy:   (batch, 5)   # Softmax probabilities
Value:    (batch, 1)   # Tanh ‚àà [-1, 1]
```

---

## üéØ Loss Function

### Formule Compl√®te

```
L(Œ∏) = L_policy(Œ∏) + L_value(Œ∏) + L_reg(Œ∏)
```

#### 1. Policy Loss (Cross-Entropy)

```
L_policy = -Œ£_a œÄ_target(a) * log(œÄ_Œ∏(a|s) + Œµ)
```

**Code** :

```python
policy_loss = -torch.sum(target_policy * torch.log(pred_policy + 1e-8), dim=1)
policy_loss = torch.mean(policy_loss)
```

#### 2. Value Loss (MSE)

```
L_value = (z - V_Œ∏(s))¬≤
```

**Code** :

```python
value_loss = (target_value - pred_value) ** 2
value_loss = torch.mean(value_loss)
```

#### 3. Regularization Loss (L2)

```
L_reg = Œª * Œ£_i Œ∏_i¬≤
```

**Code** :

```python
l2_reg = sum(p.pow(2).sum() for p in model.parameters())
reg_loss = weight_decay * l2_reg
```

**Total dans V19** :

```python
total_loss = policy_loss + value_loss + reg_loss
```

---

## ‚öôÔ∏è Optimisation

### Optimizer : Adam

**Param√®tres** :

```python
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=1e-3,          # Learning rate
    weight_decay=1e-4 # L2 regularization
)
```

**Adam Update Rule** :

```
m_t = Œ≤‚ÇÅ * m_{t-1} + (1 - Œ≤‚ÇÅ) * ‚àáL
v_t = Œ≤‚ÇÇ * v_{t-1} + (1 - Œ≤‚ÇÇ) * (‚àáL)¬≤

Œ∏_t = Œ∏_{t-1} - Œ± * mÃÇ_t / (‚àövÃÇ_t + Œµ)
```

O√π :
- **m_t** : Premier moment (moyenne des gradients)
- **v_t** : Second moment (variance des gradients)
- **Œ≤‚ÇÅ, Œ≤‚ÇÇ** : Decay rates (0.9, 0.999)
- **Œ±** : Learning rate (1e-3)

---

## üöÄ Techniques d'Entra√Ænement

### 1. Learning Rate Schedule

```python
# Initial training
lr = 1e-3

# Fine-tuning (continuous learning)
lr = 1e-3 * 0.1 = 1e-4
```

### 2. Gradient Clipping

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**Effet** : √âvite les explosions de gradients.

### 3. Batch Size

- **Initial Training** : 64
- **Evaluation** : 1 (inference single state)

---

## üìà Performance & Inference

### Temps d'Inf√©rence

- **CPU** : ~10 ms par forward pass
- **GPU (CUDA)** : ~2 ms par forward pass

### Memory Footprint

- **Model Size** : ~850 KB (221k params √ó 4 bytes)
- **Activation Memory** (batch=64) : ~200 KB

---

## üî¨ Ablation Study

### Impact des Composantes

| Variante | Sharpe Ratio | Win Rate | Notes |
|----------|--------------|----------|-------|
| **Full Model** | 1.8 | 62% | Baseline |
| Sans BatchNorm | 1.5 | 58% | Convergence plus lente |
| Sans Dropout | 1.6 | 59% | L√©ger overfitting |
| 2 Layers (128) | 1.4 | 56% | Capacit√© insuffisante |
| 4 Layers (512) | 1.7 | 61% | Pas de gain significatif |

**Conclusion** : 3√ó256 avec BN et Dropout est optimal.

---

**Prochaine section** : [04_Environment.md](04_Environment.md)
